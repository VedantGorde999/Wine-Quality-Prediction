{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmbY_kqyGbfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MFkVU8M_yQ5",
        "outputId": "823493ff-872b-4d44-d1bc-37ad293a4060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best LR: 0.817938 using {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Decision Tree: 0.812604 using {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
            "Best KNN: 0.856530 using {'algorithm': 'auto', 'n_neighbors': 7, 'weights': 'distance'}\n",
            "                 Model  Accuracy  Precision    Recall  F1 Score\n",
            "0  Logistic Regression  0.826462   0.636364  0.244444  0.353211\n",
            "1                  SVM  0.806154   0.000000  0.000000  0.000000\n",
            "2        Decision Tree  0.837538   0.642458  0.365079  0.465587\n",
            "3                  KNN  0.886154   0.722603  0.669841  0.695222\n",
            "\n",
            "Confusion Matrix for Logistic Regression\n",
            " [[1266   44]\n",
            " [ 238   77]]\n",
            "\n",
            "Confusion Matrix for SVM\n",
            " [[1310    0]\n",
            " [ 315    0]]\n",
            "\n",
            "Confusion Matrix for Decision Tree\n",
            " [[1246   64]\n",
            " [ 200  115]]\n",
            "\n",
            "Confusion Matrix for KNN\n",
            " [[1229   81]\n",
            " [ 104  211]]\n"
          ]
        }
      ],
      "source": [
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "np.random.seed(500)\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# 2. Import data\n",
        "Red = pd.read_csv('/content/winequality-red.csv', sep=';')\n",
        "White = pd.read_csv('/content/winequality-white.csv', sep=';')\n",
        "\n",
        "# 3. Concatenate the Red and White wine datasets\n",
        "Red['type'] = 'red'\n",
        "White['type'] = 'white'\n",
        "wine = pd.concat([Red, White])\n",
        "\n",
        "# Shuffling data\n",
        "wine = wine.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# 4. Create Classification version of target variable\n",
        "wine['def_quality'] = [0 if x < 7 else 1 for x in wine['quality']]  # 1 = 'Good Quality', 0 = 'Bad Quality'\n",
        "\n",
        "# Separate feature variables and target variable\n",
        "X = wine.drop(['quality', 'def_quality', 'type'], axis=1)\n",
        "Y = wine['def_quality']\n",
        "\n",
        "# 5. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=7)\n",
        "\n",
        "# 6. Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 7. Model 1: Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "penalty = ['l1','l2']\n",
        "c_values = [50, 10, 1.0, 0.1, 0.01]\n",
        "\n",
        "# Hyperparameter tuning for Logistic Regression\n",
        "grid_lr = dict(solver=solvers, penalty=penalty, C=c_values)\n",
        "Kcv = KFold(n_splits=5, shuffle=True, random_state=100)\n",
        "\n",
        "grid_search_lr = GridSearchCV(estimator=log_reg, param_grid=grid_lr, n_jobs=-1, cv=Kcv, scoring='accuracy', error_score=0)\n",
        "grid_result_lr = grid_search_lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best Logistic Regression hyperparameters\n",
        "print(\"Best LR: %f using %s\" % (grid_result_lr.best_score_, grid_result_lr.best_params_))\n",
        "log_reg_best = LogisticRegression(C=grid_result_lr.best_params_['C'], penalty=grid_result_lr.best_params_['penalty'], solver=grid_result_lr.best_params_['solver'])\n",
        "log_reg_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicting Test Set for Logistic Regression\n",
        "y_pred_log_reg = log_reg_best.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "acc_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
        "prec_log_reg = precision_score(y_test, y_pred_log_reg)\n",
        "rec_log_reg = recall_score(y_test, y_pred_log_reg)\n",
        "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
        "\n",
        "# 8. Model 2: Support Vector Machine (SVM)\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicting Test Set for SVM\n",
        "y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate SVM\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "prec_svm = precision_score(y_test, y_pred_svm)\n",
        "rec_svm = recall_score(y_test, y_pred_svm)\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "\n",
        "# 9. Model 3: Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=100)\n",
        "param_tuning_dt = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 10, 20],\n",
        "    'min_samples_leaf': [1, 5, 10]\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning for Decision Tree\n",
        "grid_search_dt = GridSearchCV(estimator=dt_model, param_grid=param_tuning_dt, n_jobs=-1, cv=Kcv, scoring='accuracy', error_score=0)\n",
        "grid_result_dt = grid_search_dt.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best Decision Tree hyperparameters\n",
        "print(\"Best Decision Tree: %f using %s\" % (grid_result_dt.best_score_, grid_result_dt.best_params_))\n",
        "dt_best = DecisionTreeClassifier(**grid_result_dt.best_params_)\n",
        "dt_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicting Test Set for Decision Tree\n",
        "y_pred_dt = dt_best.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "prec_dt = precision_score(y_test, y_pred_dt)\n",
        "rec_dt = recall_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "\n",
        "# 10. Model 4: K-Nearest Neighbors (KNN)\n",
        "knn_model = KNeighborsClassifier()\n",
        "param_tuning_knn = {\n",
        "    'n_neighbors': [3, 5, 7],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning for KNN\n",
        "grid_search_knn = GridSearchCV(estimator=knn_model, param_grid=param_tuning_knn, n_jobs=-1, cv=Kcv, scoring='accuracy', error_score=0)\n",
        "grid_result_knn = grid_search_knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best KNN hyperparameters\n",
        "print(\"Best KNN: %f using %s\" % (grid_result_knn.best_score_, grid_result_knn.best_params_))\n",
        "knn_best = KNeighborsClassifier(**grid_result_knn.best_params_)\n",
        "knn_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicting Test Set for KNN\n",
        "y_pred_knn = knn_best.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate KNN\n",
        "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
        "prec_knn = precision_score(y_test, y_pred_knn)\n",
        "rec_knn = recall_score(y_test, y_pred_knn)\n",
        "f1_knn = f1_score(y_test, y_pred_knn)\n",
        "\n",
        "# 11. Results\n",
        "results = pd.DataFrame([\n",
        "    ['Logistic Regression', acc_log_reg, prec_log_reg, rec_log_reg, f1_log_reg],\n",
        "    ['SVM', acc_svm, prec_svm, rec_svm, f1_svm],\n",
        "    ['Decision Tree', acc_dt, prec_dt, rec_dt, f1_dt],\n",
        "    ['KNN', acc_knn, prec_knn, rec_knn, f1_knn]\n",
        "], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "\n",
        "print(results)\n",
        "\n",
        "# 12. Confusion Matrices\n",
        "print(\"\\nConfusion Matrix for Logistic Regression\\n\", confusion_matrix(y_test, y_pred_log_reg))\n",
        "print(\"\\nConfusion Matrix for SVM\\n\", confusion_matrix(y_test, y_pred_svm))\n",
        "print(\"\\nConfusion Matrix for Decision Tree\\n\", confusion_matrix(y_test, y_pred_dt))\n",
        "print(\"\\nConfusion Matrix for KNN\\n\", confusion_matrix(y_test, y_pred_knn))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdVR0ejJGD2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}